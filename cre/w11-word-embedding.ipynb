{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hand-In of Group 13, Jonathan Ehrengruber (jonathan.ehrengruber@students.fhnw.ch), Christian Renold (christian.renold@hslu.ch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "1 Physical GPUs, 1 Logical GPU\n",
      "GPU Setup done\n"
     ]
    }
   ],
   "source": [
    "# setup_gpus() function for configuration the gpu on icolab jupyter hub implemented in separate file for reusability\n",
    "import gpu_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_vocabulary_from_file(filename):\n",
    "    with open(filename , 'r', encoding=\"ISO-8859-1\") as f:\n",
    "        content = f.readlines() # content is a list of lines\n",
    "        content = [x.strip() for x in content] # removing newline chars\n",
    "        content = [x for x in content if not x.startswith(';') and not x.endswith('+') and not x == ''] # remove comments, endswith-+ and empty strings\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative words: 4783\n",
      "positive words: 2005\n"
     ]
    }
   ],
   "source": [
    "negative_words = read_vocabulary_from_file('negative-words.txt')\n",
    "positive_words = read_vocabulary_from_file('positive-words.txt')\n",
    "print('negative words: {0}'.format(len(negative_words)))\n",
    "print('positive words: {0}'.format(len(positive_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get GloVe vectors: wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "def load_glove_embeddings(path):\n",
    "    embeddings = {}\n",
    "    with open(path , 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.strip().split()\n",
    "            w = values[0]\n",
    "            vectors = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings[w] = vectors\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word embeddings - load GloVe dictionary\n",
    "word_dict = load_glove_embeddings('glove.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 1500\n",
    "# had to adjust test size from 500 to 390, because we have 112 unknown words in the positive list, \n",
    "# which leads to a total of 1893 valid positive words\n",
    "test_size = 390 \n",
    "\n",
    "def get_word_vectors_for_dataset(dataset):\n",
    "    data_vector = []\n",
    "    data_words = []\n",
    "    for x in dataset:\n",
    "        try:\n",
    "            word_vector = word_dict[x]\n",
    "            data_vector.append(word_vector)\n",
    "            data_words.append(x)\n",
    "        except KeyError as e:\n",
    "            # skip unknown words\n",
    "            pass #print('KeyError {0}'.format(x))\n",
    "    old_size = len(dataset)\n",
    "    new_size = len(data_vector)\n",
    "    ignored = len(dataset) - new_size\n",
    "    print('found {0} word vectors out of {1}, ignored {2} unknown words'.format(new_size, old_size, ignored))\n",
    "    return data_vector, data_words\n",
    "\n",
    "def prepare_dataset(dataset, is_negative=0):\n",
    "    \n",
    "    np.random.shuffle(dataset) # randomize dataset, so we don't have (example) A-M in train and N-Z in test\n",
    "    vectors, words = get_word_vectors_for_dataset(dataset) # get word vectors\n",
    "    \n",
    "    # split into train and test\n",
    "    train_vectors, test_vectors, remainder = np.split(vectors, [train_size, train_size+test_size])\n",
    "    # we don't really need the words anymore for the classification, but if we want to analyse results, we have to preserve the mapping to the words\n",
    "    train_words, test_words, remainder = np.split(words, [train_size, train_size+test_size])\n",
    "    \n",
    "    train_words= train_words.reshape(train_size, 1) # reshape for concate\n",
    "    test_words = test_words.reshape(test_size, 1)\n",
    "    \n",
    "    train_x = np.concatenate((train_words, train_vectors), axis=1)\n",
    "    test_x = np.concatenate((test_words, test_vectors), axis=1)\n",
    "    \n",
    "    train_y = np.zeros((train_size, 2))\n",
    "    test_y = np.zeros((test_size, 2))\n",
    "    train_y[:,is_negative] = 1\n",
    "    test_y[:,is_negative] = 1\n",
    "    \n",
    "    train_x = np.concatenate((train_x, train_y), axis=1) # concat y-values, so we can shuffle [positive+negative] and preserve y mappings\n",
    "    test_x = np.concatenate((test_x, test_y), axis=1)\n",
    "    \n",
    "    return train_x, test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 1893 word vectors out of 2005, ignored 112 unknown words\n",
      "found 4345 word vectors out of 4783, ignored 438 unknown words\n",
      "\n",
      "Train words:  (3000,)\n",
      "X Train:  (3000, 50)\n",
      "Y Train:  (3000, 2)\n",
      "\n",
      "Test words:  (780,)\n",
      "X Test:  (780, 50)\n",
      "Y Test:  (780, 2)\n"
     ]
    }
   ],
   "source": [
    "train_data_a, test_data_a = prepare_dataset(positive_words, 0)\n",
    "train_data_b, test_data_b = prepare_dataset(negative_words, 1)\n",
    "train_data = np.concatenate((train_data_a, train_data_b))\n",
    "test_data = np.concatenate((test_data_a, test_data_b))\n",
    "\n",
    "np.random.shuffle(train_data) # shuffle, to have positive and negative words mixed\n",
    "np.random.shuffle(test_data)\n",
    "\n",
    "# extract subarrays from main array\n",
    "words_train = train_data[:,0]\n",
    "X_train = train_data[:,1:51]\n",
    "Y_train = train_data[:,51:]\n",
    "print('\\nTrain words: ', words_train.shape)\n",
    "print('X Train: ', X_train.shape)\n",
    "print('Y Train: ', Y_train.shape)\n",
    "\n",
    "words_test = test_data[:,0]\n",
    "X_test = test_data[:,1:51]\n",
    "Y_test = test_data[:,51:]\n",
    "print('\\nTest words: ', words_test.shape)\n",
    "print('X Test: ', X_test.shape)\n",
    "print('Y Test: ', Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and evaluate a classifier. Build a model, e.g. a double Dense layers in Keras\n",
    "(MLP) and train it. Report on the evolution of the loss and accuracy along the epochs.\n",
    "You should reach about 90% accuracy on the training set and 85% accuracy on the test\n",
    "set. Report on your model structure and fitting strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding\n",
    "from tensorflow.keras.initializers import Constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_13 (Dense)             (None, 100)               5100      \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 2)                 202       \n",
      "=================================================================\n",
      "Total params: 5,302\n",
      "Trainable params: 5,302\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    # Embedding(3000, 50, input_length=50, trainable=False),\n",
    "    Dense(units=100, input_shape=(50,), activation='relu'),\n",
    "    Dense(units=2, activation='softmax')\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3000 samples, validate on 780 samples\n"
     ]
    }
   ],
   "source": [
    "log = model.fit(X_train, Y_train, validation_data=(X_test, Y_test), verbose=1) # , batch_size=128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "I prepared the data so that `X_train` is a `(3000, 50)` array with each row representing a word with the vector from the GloVe word embeddings.\n",
    "\n",
    "The Kernel seems to get stuck on the above code block on `model.fit()` and I can't figure out why. \n",
    "Unfortunately I couldn't find any hints on the lecture notes or on the internet on how to build such a model to get a word classifier \n",
    "and because of that I wasn't able to debug the error above. Therefor the exercise could not be completed properly.\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
