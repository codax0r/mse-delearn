{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hand-In of Group 13, Jonathan Ehrengruber (jonathan.ehrengruber@students.fhnw.ch), Christian Renold (christian.renold@hslu.ch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Data\n",
    "\n",
    "Binary classification based on MNIST data. \n",
    "\n",
    "It restricts the classification problem to two digits, selects them from the MNIST dataset, splits it up into a train and test part and then trains a binary classification (logistic regression) to learn to differentiate between the two digits.\n",
    "\n",
    "Either the original MNIST dataset with 28x28 images or a smaller light version with 8x8 images can be used. \n",
    "\n",
    "The following notation is used: <br>\n",
    "<code>m</code>: Number of samples <br>\n",
    "<code>n</code>: Number of features\n",
    "\n",
    "Here the features refer to the pixel values of the images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Folder\n",
    "\n",
    "The data can be loaded by using suitable functionality in sklearn which will use a dedicated folder on your local disk for caching. Specify the folder to be used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START YOUR CODE ###\n",
    "\n",
    "data_home = \"/home/jovyan/work/cre/data/\"\n",
    "\n",
    "### END YOUR CODE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "\n",
    "Some preparatory steps to be applied before training:\n",
    "* Loading the data\n",
    "* Some plots\n",
    "* Extracting two digits and restricting the classification task to that so that the dataset is well balanced.\n",
    "* Splitting the dataset into train and test\n",
    "* Rescaling the intensities to the range [0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_img(img, label, shape):\n",
    "    \"\"\"\n",
    "    Plot the x array by reshaping it into a square array of given shape\n",
    "    and print the label.\n",
    "    \n",
    "    Parameters:\n",
    "    img -- array with the intensities to be plotted of shape (shape[0]*shape[1])\n",
    "    label -- label \n",
    "    shape -- 2d tuple with the dimensions of the image to be plotted.\n",
    "    \"\"\"\n",
    "    plt.imshow(np.reshape(img, shape), cmap=plt.cm.gray)\n",
    "    plt.title(\"Label %i\"%label)\n",
    "\n",
    "\n",
    "def plot_digits(x,y,selection,shape,selected_digits, cols=5):\n",
    "    \"\"\"\n",
    "    Plots the digits in a mosaic with given number of columns.\n",
    "\n",
    "    Arguments:\n",
    "    x -- array of images of size (n,m)\n",
    "    y -- array of labels of size (1,m)\n",
    "    selection -- list of selection of samples to be plotted\n",
    "    shape -- shape of the images (a 2d tuple)\n",
    "    selected_digits -- tuple with the two selected digits (the first associated with label 1, the second with label 0)\n",
    "    \"\"\"\n",
    "    if len(selection)==0:\n",
    "        print(\"No images in the selection!\")\n",
    "        return\n",
    "    cols = min(cols, len(selection))\n",
    "    rows = len(selection)/cols+1\n",
    "    plt.figure(figsize=(20,4*rows))\n",
    "    digit1 = selected_digits[0]\n",
    "    digit2 = selected_digits[1]    \n",
    "    for index, (image, label) in enumerate(zip(x.T[selection,:], y.T[selection,:])):\n",
    "        digit = digit1 if label==1 else digit2\n",
    "        plt.subplot(rows, cols, index+1)\n",
    "        plt.imshow(np.reshape(image, shape), cmap=plt.cm.gray)\n",
    "        plt.title('Sample %i\\n Label %i\\n' % (selection[index],digit), fontsize = 12)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data\n",
    "\n",
    "Follow the instructions in the doc string of the <span style=\"color:blue\">load_mnist</span>-method defined below so that you can load the \"MNIST original\" dataset.\n",
    "\n",
    "Load the data MNIST dataset and plot the 17th image by using the <span style=\"color:blue\">plot_image</span>-method defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "\n",
    "def load_mnist(data_home):\n",
    "    \"\"\"\n",
    "    Loads the mnist dataset, prints the shape of the dataset and \n",
    "    returns the array with the images, the array with associated labels \n",
    "    and the shape of the images.\n",
    "    \n",
    "    Parameters: \n",
    "    data_home -- Absolute path to the DATA_HOME  \n",
    "    \n",
    "    Returns:\n",
    "    x -- array with images of shape (784,m) where m is the number of images\n",
    "    y -- array with associated labels with shape (1,m) where m is the number of images\n",
    "    shape -- (28,28)\n",
    "    \"\"\"\n",
    "    mnist = fetch_openml('mnist_784', data_home=data_home)\n",
    "    x, y = mnist['data'].T, np.array(mnist['target'], dtype='int').T\n",
    "    m = x.shape[1]\n",
    "    y = y.reshape(1,m)\n",
    "    print(\"Loaded MNIST original:\")\n",
    "    print(\"Image Data Shape\" , x.shape)\n",
    "    print(\"Label Data Shape\", y.shape)\n",
    "    return x,y,(28,28)\n",
    "\n",
    "\n",
    "### START YOUR CODE ###\n",
    "\n",
    "mnist_x,mnist_y,mnist_shape = load_mnist(data_home)\n",
    "plot_img(mnist_x[:,16], mnist_y[0, 16], mnist_shape)\n",
    "\n",
    "### END YOUR CODE ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split Data and bring it in the correct shape\n",
    "\n",
    "Split the data into training set and test set.\n",
    "We use the scikit-learn function 'train_test_split' and use a (80%/20%) splitting.\n",
    "\n",
    "Furthermore, we bring the input data (x) into the shape (n,m) where n is the number of input features and m the number of samples.\n",
    "\n",
    "Load the MNIST dataset (by using <span style=\"color:blue\">load_mnist</span> from above), filter it to only use the digits '1' and '7' (by using the method <span style=\"color:blue\">filter_digits</span> and split up the result further into a training and a test set (by using the <span style=\"color:blue\">prepare_train_test</span>). Use a 80-20 split of the data into train and test.\n",
    "\n",
    "As a result, you can run the test which should not produce any exception."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def filter_digits(x, y, selected_digits):\n",
    "    \"\"\"\n",
    "    Filter the dataset for given two digits (label values between 0 and 9).\n",
    "    The samples with the first digit will be associated with the label 1, the second with 0. \n",
    "    \n",
    "    Parameters:\n",
    "    x -- Array of images of shape (n,m) where m is the number of samples\n",
    "    y -- Array of labels of shape (1,m) where m is the number of samples\n",
    "    digits -- tuple with the two digit values to filter for\n",
    "    \n",
    "    Returns:\n",
    "    x1 -- filtered list of images of shape (n,m1) with m1 the number of samples \n",
    "    y1 -- filtered list of labels of shape (1,m1)\n",
    "    \"\"\"\n",
    "    # select two given digits - will the train a model that learns to differentiate between the two\n",
    "    digit1 = selected_digits[0]\n",
    "    digit2 = selected_digits[1]\n",
    "    mask1 = y[0,:]==digit1\n",
    "    mask2 = y[0,:]==digit2\n",
    "    x1 = x[:,mask1 | mask2]\n",
    "    y1 = y[0,mask1 | mask2]\n",
    "    y1 = y1.reshape(1,y1.size)\n",
    "    \n",
    "    ## Define the label for the binary classification\n",
    "    mask1 = y1[0,:]==digit1\n",
    "    mask2 = y1[0,:]==digit2\n",
    "    y1[0,mask1] = 1\n",
    "    y1[0,mask2] = 0\n",
    "\n",
    "    print(\"Selecting %i images with digit %i and %i images with digit %i\"%(np.sum(mask1),digit1,np.sum(mask2),digit2))\n",
    "    return x1,y1\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def prepare_train_test(x, y, test_size=0.20):\n",
    "    \"\"\"\n",
    "    Split the dataset consisting of an array of images (shape (m,n)) and an array of labels (shape (n,))\n",
    "    into train and test set.\n",
    "    \n",
    "    Parameters:\n",
    "    x -- Array of images of shape (n,m) where m is the number of samples\n",
    "    y -- Array of labels of shape (m,) where m is the number of samples\n",
    "    test_size -- fraction of samples to reserve as test sample\n",
    "    \n",
    "    Returns:\n",
    "    x_train -- list of images of shape (n,m1) used for training\n",
    "    y_train -- list of labels of shape (1,m1) used for training\n",
    "    x_test -- list of images of shape (n,m2) used for testing\n",
    "    y_test -- list of labels of shape (1,m2) used for testing\n",
    "    \"\"\"\n",
    "    # split \n",
    "    # We use the functionality of sklearn which assumes that the samples are enumerated with the first index \n",
    "    x_train, x_test, y_train, y_test = train_test_split(x.T, y.T, test_size=0.20, random_state=1)\n",
    "\n",
    "    # reshape - transpose back the output obtained from the train_test_split-function\n",
    "    x_train = x_train.T\n",
    "    x_test = x_test.T\n",
    "    m_train = x_train.shape[1]\n",
    "    m_test = x_test.shape[1]\n",
    "    y_train=y_train.reshape(1,m_train)\n",
    "    y_test=y_test.reshape(1,m_test)\n",
    "\n",
    "    print(\"Shape training set: \", x_train.shape, y_train.shape)\n",
    "    print(\"Shape test set:     \", x_test.shape, y_test.shape)\n",
    "    \n",
    "    return x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START YOUR CODE ###\n",
    "\n",
    "selected_digits = (1,7)\n",
    "x_filtered, y_filtered = filter_digits(mnist_x, mnist_y, selected_digits)\n",
    "x_train, x_test, y_train, y_test = prepare_train_test(x_filtered, y_filtered)\n",
    "\n",
    "### END YOUR CODE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TEST ##\n",
    "np.testing.assert_array_equal(x_train.shape, (784, 12136))\n",
    "np.testing.assert_array_equal(y_train.shape, (1, 12136))\n",
    "print('test ok')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Normalisation\n",
    "\n",
    "Rescale the data - apply min/max rescaling (- we get back to centralisation later).\n",
    "\n",
    "Test that the result is expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def rescale(x_train,x_test):\n",
    "    \"\"\"\n",
    "    Rescales to samples to values within [0,1] - min and max values computed from the training set.\n",
    "    The min and max are computed over all samples and features.\n",
    "    \n",
    "    Parameters:\n",
    "    x_train -- Array of training samples of shape (n,m1) where n,m1 are the number of features and samples, respectively.  \n",
    "    x_test -- Array of test samples of shape (n,m2) where n,m2 are the number of features and samples, respectively. \n",
    "    \n",
    "    Returns:\n",
    "    The arrays with the rescaled train and test samples.  \n",
    "    \"\"\"\n",
    "    ### START YOUR CODE ###    \n",
    "\n",
    "    train_min = np.amin(x_train)\n",
    "    train_max = np.amax(x_train)    \n",
    "    train_maxmin_diff = train_max - train_min\n",
    "    \n",
    "    x_train = (x_train - train_min) / train_maxmin_diff\n",
    "    x_test = (x_test - train_min) / train_maxmin_diff\n",
    "    \n",
    "    ### END YOUR CODE ###\n",
    "    return x_train, x_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TEST ##\n",
    "x_train = np.array([0,3,2,5,10,9]).reshape(1,6)\n",
    "x_test = np.array([11,20,1,-1]).reshape(1,4)\n",
    "x1,x2 = rescale(x_train, x_test)\n",
    "np.testing.assert_array_almost_equal(x1,np.array([0.,0.3,0.2,0.5,1.,0.9]).reshape(1,6),decimal=8)\n",
    "np.testing.assert_array_almost_equal(x2,np.array([1.1,2.0,0.1,-0.1]).reshape(1,4),decimal=8)\n",
    "print('test ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_digits = (1,7)\n",
    "x,y, shape = load_mnist(data_home)\n",
    "x1, y1 = filter_digits(x,y,selected_digits)\n",
    "x_train1, x_test1, y_train, y_test = prepare_train_test(x1, y1, test_size=0.20)\n",
    "x_train,x_test = rescale(x_train1,x_test1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of z\n",
    "    \n",
    "    Parameters:\n",
    "    z -- A scalar or numpy array of any size.\n",
    "    Return:\n",
    "    s -- sigmoid(z)\n",
    "    \"\"\"\n",
    "    ### START YOUR CODE ### \n",
    "    return 1 / (1 + np.exp(-z))\n",
    "    ### END YOUR CODE ### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TEST ##\n",
    "z = np.array([1,-2,2,0]).reshape(1,4)\n",
    "y = sigmoid(z)\n",
    "ytrue = np.array([0.73105858, 0.11920292, 0.88079708, 0.5]).reshape(1,4)\n",
    "np.testing.assert_array_almost_equal(y,ytrue,decimal=8)\n",
    "print('test ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def predict(w, b, X):\n",
    "    '''\n",
    "    Compute the prediction for each of the m samples by using the parameters (w, b).\n",
    "    \n",
    "    Parameters:\n",
    "    w -- weights, a numpy array with shape (1, n)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of size (n,m)\n",
    "    \n",
    "    Returns:\n",
    "    predictions -- a numpy array (vector) containing all predictions\n",
    "    '''\n",
    "    ### START YOUR CODE ### \n",
    "    \n",
    "    return sigmoid(w.dot(X) + b)\n",
    "    \n",
    "    ### END YOUR CODE ###     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TEST ##\n",
    "X = np.array([1,-2,2,1]).reshape(4,1)\n",
    "w = np.array([1,1,0.75,0]).reshape(1,4)\n",
    "b = -0.25\n",
    "\n",
    "y = predict(w,b,X)\n",
    "ytrue = np.array([sigmoid(0.25)]).reshape(1,1)\n",
    "np.testing.assert_array_almost_equal(y,ytrue,decimal=8)\n",
    "print('test ok')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Function\n",
    "\n",
    "* Cross-Entropy Cost Function\n",
    "* Mean Square Error Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def cost_CE(ypred, y, eps=1.0e-12):\n",
    "    \"\"\"\n",
    "    Computes the cross entropy cost function for given predicted values and labels.\n",
    "    It clips (using numpy clip) predicted values to be in the interval [eps,1-eps] so that numerical \n",
    "    issues with the calculation of logarithm are avoided.\n",
    "    \n",
    "    Parameters:\n",
    "    ypred -- Predicted values, a numpy array with shape (1,m).\n",
    "    y -- Ground truth values (labels 0 or 1), a numpy array with shape (1,m)\n",
    "    \n",
    "    Returns:\n",
    "    Cross Entropy Cost\n",
    "    \"\"\"\n",
    "    # sanity checks:\n",
    "    try:\n",
    "        if ypred.shape != y.shape:\n",
    "            raise AttributeError(\"The two input arguments ypred and y should be numpy arrays of the same shape.\")\n",
    "    except Exception:\n",
    "        raise AttributeError(\"Wrong type of argument - ypred and y should be a numpy array\")\n",
    "\n",
    "    # clip predicted values and compute the cost\n",
    "    \n",
    "    ### START YOUR CODE ### \n",
    "    \n",
    "    ypred_clipped = np.clip(ypred, eps, 1-eps)    \n",
    "    J = -np.mean(( y * np.log(ypred_clipped)) + ((1-y) * np.log(1-ypred_clipped)))\n",
    "        \n",
    "    ### END YOUR CODE ### \n",
    "    \n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TEST ##\n",
    "\n",
    "# CASE 1: Numeric value computed correctly\n",
    "yhat = np.array([0.1,0.2,0.5,0.8,0.9,1.0]).reshape(1,6)\n",
    "y = np.array([0,1,1,0,1,1]).reshape(1,6)\n",
    "J = cost_CE(yhat,y)\n",
    "Jtrue = -(np.log(0.2)+np.log(0.5)+np.log(0.9)+np.log(1.0)+np.log(0.9)+np.log(0.2))/6\n",
    "np.testing.assert_almost_equal(J,Jtrue,decimal=8)\n",
    "\n",
    "# CASE 2: Both arguments should be numpy arrays of the same shape\n",
    "try:\n",
    "    cost_CE(1,1)\n",
    "except AttributeError:\n",
    "    print(\"Exception ok\")\n",
    "    \n",
    "# CASE 3: Both arguments should be numpy arrays of the same shape\n",
    "try:\n",
    "    cost_CE(yhat,1)\n",
    "except AttributeError:\n",
    "    print(\"Exception ok\")\n",
    "print('test ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def cost_MSE(ypred, y):\n",
    "    \"\"\"\n",
    "    Computes the mean square error cost function for given predicted values and labels.\n",
    "    \n",
    "    Parameters:\n",
    "    ypred -- Predicted values, a numpy array with shape (1,m).\n",
    "    y -- Ground truth values (labels 0 or 1), a numpy array with shape (1,m)\n",
    "    \n",
    "    Returns:\n",
    "    MSE Cost\n",
    "    \"\"\"    \n",
    "    # sanity checks:\n",
    "    try:\n",
    "        if ypred.shape != y.shape:\n",
    "            raise AttributeError(\"The two input arguments ypred and y should be numpy arrays of the same shape.\")\n",
    "    except Exception:\n",
    "        raise AttributeError(\"Wrong type of argument - ypred and y should be a numpy array\")\n",
    "\n",
    "    ### START YOUR CODE ### \n",
    "    \n",
    "    J =  np.mean(np.power(ypred - y, 2))\n",
    "    \n",
    "    ### END YOUR CODE ### \n",
    "    \n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TEST ##\n",
    "\n",
    "# CASE 1: Numeric value computed correctly\n",
    "yhat = np.array([0.1,0.2,0.5,0.8,0.9,1.0]).reshape(1,6)\n",
    "y = np.array([0,1,1,0,1,1]).reshape(1,6)\n",
    "J = cost_MSE(yhat,y)\n",
    "Jtrue = (0.01+0.64+0.25+0.64+0.01)/6\n",
    "np.testing.assert_almost_equal(J,Jtrue,decimal=8)\n",
    "\n",
    "# CASE 2: Both arguments should be numpy arrays of the same shape\n",
    "try:\n",
    "    cost_MSE(1,1)\n",
    "except AttributeError:\n",
    "    print(\"Exception ok\")\n",
    "    \n",
    "# CASE 3: Both arguments should be numpy arrays of the same shape\n",
    "try:\n",
    "    cost_MSE(yhat,1)\n",
    "except AttributeError:\n",
    "    print(\"Exception ok\")\n",
    "print('test ok')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update Rules for the Parameters\n",
    "\n",
    "Different update rules associated with the different cost functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def step_CE(X, Y, Ypred):\n",
    "    \"\"\"\n",
    "    Computes the update of the weights and bias from the gradient of the cross entropy cost. \n",
    "    \n",
    "    Arguments:\n",
    "    X -- data of size (n, m) where n is the number of input features and m the number of samples.\n",
    "    Y -- label vector (1, m) where m the number of samples.\n",
    "    Ypred -- predicted scores (1, m)\n",
    "\n",
    "    Returns: \n",
    "    Dictionary with the gradient w.r.t. weights ('dw') and w.r.t. bias ('db')\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START YOUR CODE ### \n",
    "    \n",
    "    _, m = X.shape\n",
    "    Ydiff = (Ypred - Y)\n",
    "    dw = Ydiff@X.T * 1/m\n",
    "    db = np.mean(Ydiff, 1)\n",
    "    \n",
    "    ### END YOUR CODE ### \n",
    "    \n",
    "    return {\"dw\": dw, \"db\": db}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TEST ##\n",
    "\n",
    "x = np.array([[1,2,3],[4,5,6]]).reshape(2,3)\n",
    "y = np.array([1,0,1]).reshape(1,3)\n",
    "ypred = np.array([0.8,0.3,0.9]).reshape(1,3)\n",
    "\n",
    "res = step_CE(x,y,ypred)\n",
    "dwtrue = np.array([0.033333333,0.033333333]).reshape(1,2)\n",
    "np.testing.assert_almost_equal(res[\"dw\"],dwtrue,decimal=8)\n",
    "print('test ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def step_MSE(X, Y, Ypred):\n",
    "    \"\"\"\n",
    "    Computes the update of the weights and bias from the gradient of the mean square error cost. \n",
    "\n",
    "    Arguments:\n",
    "    X -- data of size (n, m) where n is the number of input features and m the number of samples.\n",
    "    Y -- label vector (1, m) where m the number of samples.\n",
    "    Ypred -- predicted scores (1, m)\n",
    "\n",
    "    Returns:\n",
    "    Dictionary with the gradient w.r.t. weights ('dw') and w.r.t. bias ('db')\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START YOUR CODE ### \n",
    "    \n",
    "    n,m = X.shape\n",
    "    Ydiff = Ypred - Y\n",
    "    Yinv = 1 - Ypred\n",
    "    a = Ypred * Yinv\n",
    "    b = a * Ydiff\n",
    "    bias = Ypred * Yinv * Ydiff\n",
    "    dw = np.sum(bias @ X.T, axis=0).reshape(1,n) * 1/m\n",
    "    db = np.mean(bias)\n",
    "    \n",
    "    ### END YOUR CODE ### \n",
    "    \n",
    "    return {\"dw\": dw, \"db\": db}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TEST ##\n",
    "\n",
    "x = np.array([[1,2,3],[4,5,6]]).reshape(2,3)\n",
    "y = np.array([1,0,1]).reshape(1,3)\n",
    "ypred = np.array([0.8,0.3,0.9]).reshape(1,3)\n",
    "\n",
    "res = step_MSE(x,y,ypred)\n",
    "dwtrue = np.array([0.02233333,0.04433333]).reshape(1,2)\n",
    "np.testing.assert_almost_equal(res[\"dw\"],dwtrue,decimal=8)\n",
    "print('test ok')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics for measuring the performance of the algorithm  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def error_rate(Ypred, Y):\n",
    "    \"\"\"\n",
    "    Computes the error rate defined as the fraction of misclassified samples.\n",
    "    \n",
    "    Arguments:\n",
    "    Ypred -- predicted scores with values in [0,1], array of shape (1,m)\n",
    "    Y -- ground truth labels with values in {0,1}, array of shape (1,m)\n",
    "\n",
    "    Returns:\n",
    "    error_rate \n",
    "    \"\"\"\n",
    "    \n",
    "    return np.sum(Y != np.round(Ypred)) / Y.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TEST ##\n",
    "y = np.array([1,0,1,1,0])\n",
    "ypred = np.array([0.9,0.1,0.4,0.8,0.7])\n",
    "np.testing.assert_almost_equal(error_rate(ypred, y),0.4,decimal=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize and Optimize (Learn)\n",
    "\n",
    "#### Initialize Parameters\n",
    "\n",
    "First we provide a utility method to generate properly intialized parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def initialize_params(n, random=False):\n",
    "    \"\"\"\n",
    "    This function provides initialized parameters: a vector of shape (1,n) as weights and a scalar equal to zero as bias. \n",
    "    \n",
    "    Argument:\n",
    "    n -- size of the w vector we want (number of features)\n",
    "    random -- if set to True stand norma distributed values are set for the weights; otherwise zeros are used.\n",
    "    \n",
    "    Returns:\n",
    "    w -- initialized vector of shape (1,n)\n",
    "    b -- initialized scalar (corresponds to the bias)\n",
    "    \"\"\"\n",
    "    if random:\n",
    "        w = np.random.randn(*(1,n))\n",
    "    else:\n",
    "        w = np.zeros((1,n))\n",
    "    \n",
    "    b = 0.0\n",
    "    \n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TEST ##\n",
    "w0, b0 = initialize_params(100)\n",
    "np.testing.assert_array_equal(w0.shape, (1,100))\n",
    "\n",
    "w0, b0 = initialize_params(100, random=True)\n",
    "np.testing.assert_array_equal(w0.shape, (1,100))\n",
    "np.testing.assert_almost_equal(np.mean(w0),0.0,decimal=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metrics Class\n",
    "\n",
    "For not littering the optimization loop with code to keep track of the learning results over the epochs we defined a suitable metrics object that keeps all the data (cost function, classification error vs epochs). It also provides utility methods for updating, printing values or plotting the learning curves.\n",
    "\n",
    "It is defined as python class the metrics object then needs to be instantiated from. It means that some small knowledge about object-oriented programming is needed here.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metrics():\n",
    "    \"\"\"\n",
    "    Allows to collect statistics (such as classification error or cost) that are of interest over the course of training\n",
    "    and for creating learning curves that are a useful tool for analyzing the quality of the learning.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, cost_function=cost_CE):\n",
    "        \"\"\"\n",
    "        Constructor for a metrics object. \n",
    "        Initializes all the statistics to track in form of python lists.\n",
    "        \n",
    "        Parameters:\n",
    "        cost_function -- a function object that allows to compute the cost.\n",
    "        \"\"\"\n",
    "        self.epochs = []\n",
    "        self.train_costs = []\n",
    "        self.test_costs = []\n",
    "        self.train_errors = []\n",
    "        self.test_errors = []\n",
    "        self.stepsize_w = []\n",
    "        self.stepsize_b = []\n",
    "        self.cost_function = cost_function\n",
    "    \n",
    "    def update(self, epoch, ypred_train, y_train, ypred_test, y_test, dw, db):\n",
    "        \"\"\"\n",
    "        Allows to update the statistics to be tracked for a new epoch.\n",
    "        The cost is computed by using the function object passed to the constructor.\n",
    "        \n",
    "        Parameters:\n",
    "        epoch -- Epoch\n",
    "        ypred_train -- predicted values on the training samples, a numpy array of shape (1,m1)\n",
    "        y_train -- ground truth labels associated with the training samples, a numpy array of shape (1,m1)\n",
    "        ypred_test -- predicted values on the test samples, a numpy array of shape (1,m2)\n",
    "        y_test -- ground truth labels associated with the test samples, a numpy array of shape (1,m2)\n",
    "        dw -- some lenght measure for the gradient w.r.t. the weights, a numpy array of shape (1,n)\n",
    "        db -- gradient w.r.t. the bias, a scalar\n",
    "        \"\"\"\n",
    "        Jtrain = self.cost_function(ypred_train, y_train)\n",
    "        Jtest = self.cost_function(ypred_test, y_test)\n",
    "        train_error = error_rate(ypred_train, y_train)\n",
    "        test_error = error_rate(ypred_test, y_test)\n",
    "\n",
    "        self.epochs.append(epoch)\n",
    "        self.train_costs.append(Jtrain)\n",
    "        self.test_costs.append(Jtest)\n",
    "        self.train_errors.append(train_error)\n",
    "        self.test_errors.append(test_error)\n",
    "        self.stepsize_w.append(dw)\n",
    "        self.stepsize_b.append(db)\n",
    "        \n",
    "    def print_latest_errors(self):\n",
    "        print (\"Train/test error after epoch %i: %f, %f\" %(self.epochs[-1], self.train_errors[-1], self.test_errors[-1]))\n",
    "\n",
    "    def print_latest_costs(self):\n",
    "        print (\"Train/test cost after epoch %i: %f, %f\" %(self.epochs[-1], self.train_costs[-1], self.test_costs[-1]))\n",
    "\n",
    "    def plot_cost_curves(self, ymin=None, ymax=None):\n",
    "        plt.semilogy(self.epochs, self.train_costs, label=\"train\")\n",
    "        plt.semilogy(self.epochs, self.test_costs, label=\"test\")\n",
    "        plt.ylabel('Cost')\n",
    "        plt.xlabel('Epochs')\n",
    "        xmax = self.epochs[-1]\n",
    "        if not ymin:\n",
    "            ymin = min(max(1e-5,np.min(self.train_costs)),max(1e-5,np.min(self.test_costs))) * 0.8\n",
    "        if not ymax:\n",
    "            ymax = max(np.max(self.train_costs),np.max(self.test_costs)) * 1.2\n",
    "        plt.axis([0,xmax,ymin,ymax])\n",
    "        plt.legend()\n",
    "        plt.show()        \n",
    "    \n",
    "    def plot_error_curves(self, ymin=None, ymax=None):\n",
    "        plt.semilogy(self.epochs, self.train_errors, label=\"train\")\n",
    "        plt.semilogy(self.epochs, self.test_errors, label=\"test\")\n",
    "        plt.ylabel('Errors')\n",
    "        plt.xlabel('Epochs')\n",
    "        xmax = self.epochs[-1]\n",
    "        if not ymin:\n",
    "            ymin = min(max(1e-5,np.min(self.train_errors)),max(1e-5,np.min(self.test_errors))) * 0.8\n",
    "        if not ymax:\n",
    "            ymax = max(np.max(self.train_errors),np.max(self.test_errors)) * 1.2\n",
    "        plt.axis([0,xmax,ymin,ymax])\n",
    "        plt.legend()\n",
    "        plt.show()        \n",
    "\n",
    "    def plot_stepsize_curves(self, ymin=None, ymax=None):\n",
    "        plt.semilogy(self.epochs, self.stepsize_w, label=\"dw\")\n",
    "        plt.semilogy(self.epochs, self.stepsize_b, label=\"db\")\n",
    "        plt.ylabel('Step Sizes (dw,db)')\n",
    "        plt.xlabel('Epochs')\n",
    "        xmax = self.epochs[-1]\n",
    "        if not ymin:\n",
    "            ymin = min(max(1e-5,np.min(self.stepsize_w)),max(1e-5,np.min(self.stepsize_b))) * 0.8\n",
    "        if not ymax:\n",
    "            ymax = max(np.max(self.stepsize_w),np.max(self.stepsize_b)) * 1.2\n",
    "        plt.axis([0,xmax,ymin,ymax])\n",
    "        plt.legend()\n",
    "        plt.show()        \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(Metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def optimize(w, b, x_train, y_train, x_test, y_test, nepochs, alpha, cost_type=\"CE\", debug = False):\n",
    "    \"\"\"\n",
    "    This function optimizes w and b by running (batch) gradient descent.\n",
    "    \n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (1,n)\n",
    "    b -- bias, a scalar\n",
    "    x -- array of samples of shape (n,m)\n",
    "    y -- ground truth labels vector (containing 0 or 1) of shape (1, m)\n",
    "    nepochs -- number of iterations of the optimization loop\n",
    "    alpha -- learning rate of the gradient descent update rule\n",
    "    cost -- type of cost function to use for the opimisation (CE or MSE)\n",
    "    debug -- True to print the loss every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    params -- dictionary containing the weights w and bias b\n",
    "    metrics -- Metrics object that contains metrics collected while training was in progress    \n",
    "    \"\"\" \n",
    "    if \"CE\" == cost_type:\n",
    "        cost_function = cost_CE\n",
    "        step_function = step_CE\n",
    "    else:\n",
    "        cost_function = cost_MSE\n",
    "        step_function = step_MSE\n",
    "    \n",
    "    metrics = Metrics(cost_function=cost_function)\n",
    "\n",
    "    # compute and set the initial values for the metrics curves\n",
    "    ypred_train = predict(w,b,x_train)\n",
    "    ypred_test = predict(w,b,x_test)    \n",
    "    metrics.update(0, ypred_train, y_train, ypred_test, y_test, 0, 0)\n",
    "    \n",
    "    ### START YOUR CODE ### \n",
    "    \n",
    "    # Loop over the epochs, don't forget to do the metrics.update(....) per epoch\n",
    "    \n",
    "    for epoch in np.arange(1, nepochs):\n",
    "        ypred_train = predict(w,b, x_train)\n",
    "        ypred_test = predict(w,b, x_test)\n",
    "        \n",
    "        cost = cost_function(ypred_train, y_train)\n",
    "        gradient = step_function(x_train, y_train, ypred_train)\n",
    "        dw = gradient['dw']\n",
    "        db = gradient['db']\n",
    "        w -= (alpha * dw)\n",
    "        b -= (alpha * db)\n",
    "        metrics.update(epoch, ypred_train, y_train, ypred_test, y_test, gradient['dw'], gradient['db'])\n",
    "\n",
    "    \n",
    "    ### START YOUR CODE ### \n",
    "        \n",
    "    # finally, we print the latest metrics values and return\n",
    "    metrics.print_latest_costs()\n",
    "    metrics.print_latest_errors()\n",
    "\n",
    "    return {\"w\": w, \"b\": b}, metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Training for Specific Setting\n",
    "\n",
    "Compose that all in a single \"pipeline\" starting with all the steps of the data preparation followed by the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "selected_digits = (1,7)\n",
    "x,y, shape = load_mnist(data_home)\n",
    "x1, y1 = filter_digits(x,y,selected_digits)\n",
    "x_train, x_test, y_train, y_test = prepare_train_test(x1, y1, test_size=0.20)\n",
    "x_train,x_test = rescale(x_train1,x_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### START YOUR CODE ###\n",
    "\n",
    "# preparing data\n",
    "\n",
    "\n",
    "# training and testing\n",
    "learning_rate = 0.25\n",
    "nepochs = 1000\n",
    "nfeatures = x_train.shape[0]\n",
    "debug = False\n",
    "w,b = initialize_params(nfeatures) # initial parameters\n",
    "params, metrics = optimize(w, b, x_train, y_train, x_test, y_test, nepochs, learning_rate, \"CE\", debug)\n",
    "\n",
    "### END YOUR CODE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Learning Curves\n",
    "\n",
    "Cost <br>\n",
    "Error Rate <br>\n",
    "Learning Speed (lenght of parameter change - norm of difference in the weights matrix, norm of difference in the bias )<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "metrics.plot_cost_curves()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metrics.plot_error_curves(ymin=0.0015,ymax=0.1)\n",
    "min(metrics.test_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO \n",
    "# ValueError: setting an array element with a sequence.\n",
    "metrics.plot_stepsize_curves()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Misclassified Digits\n",
    "\n",
    "Use the <span style=\"color:blue\">plot_digits</span>- method to plot the misclassified digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### START YOUR CODE ###\n",
    "\n",
    "y_final_pred_values = predict(params['w'],params['b'], x_test)\n",
    "# mask = y_test != np.round(y_final_pred_values)\n",
    "mask2 = np.argwhere(y_test != np.round(y_final_pred_values))[:, 1]\n",
    "plot_digits(x_test,y_test,mask2,shape,selected_digits, cols=5)\n",
    "\n",
    "### END YOUR CODE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(np.reshape(params['w'], shape), cmap=plt.cm.gray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "answer questions and play more with settings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
