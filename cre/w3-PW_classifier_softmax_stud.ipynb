{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hand-In of Group 13, Jonathan Ehrengruber (jonathan.ehrengruber@students.fhnw.ch), Christian Renold (christian.renold@hslu.ch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Data\n",
    "\n",
    "Full classification of MNIST data.\n",
    "\n",
    "The original MNIST dataset is used.\n",
    "\n",
    "The following notation is used: <br>\n",
    "<code>m</code>: Number of samples <br>\n",
    "<code>n</code>: Number of features\n",
    "\n",
    "Here the features refer to the pixel values of the images.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Folder\n",
    "\n",
    "The data can be loaded by using suitable functionality in sklearn which will use a dedicated folder on your local disk for caching. Specify the folder to be used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START YOUR CODE ### \n",
    "data_home = \n",
    "### END YOUR CODE ### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "\n",
    "Some preparatory steps to be applied before training:\n",
    "* Loading the data\n",
    "* Some plots\n",
    "* Extracting two digits and restricting the classification task to that so that the dataset is well balanced.\n",
    "* Splitting the dataset into train and test\n",
    "* Normalizing the intensities to the range [-1,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_img(img, label, shape):\n",
    "    \"\"\"\n",
    "    Plot the x array by reshaping it into a square array of given shape\n",
    "    and print the label.\n",
    "    \n",
    "    Parameters:\n",
    "    img -- array with the intensities to be plotted of shape (shape[0]*shape[1])\n",
    "    label -- label \n",
    "    shape -- 2d tuple with the dimensions of the image to be plotted.\n",
    "    \"\"\"\n",
    "    plt.imshow(np.reshape(img, shape), cmap=plt.cm.gray)\n",
    "    plt.title(\"Label %i\"%label)\n",
    "\n",
    "\n",
    "def plot_digits(x,y,selection,shape, cols=5):\n",
    "    \"\"\"\n",
    "    Plots the digits in a mosaic with given number of columns.\n",
    "\n",
    "    Arguments:\n",
    "    x -- array of images of size (n,m)\n",
    "    y -- array of labels of size (1,m)\n",
    "    selection -- list of selection of samples to be plotted\n",
    "    shape -- shape of the images (a 2d tuple)\n",
    "    selected_digits -- tuple with the two selected digits (the first associated with label 1, the second with label 0)\n",
    "    \"\"\"\n",
    "    if len(selection)==0:\n",
    "        print(\"No images in the selection!\")\n",
    "        return\n",
    "    cols = min(cols, len(selection))\n",
    "    rows = len(selection)/cols+1\n",
    "    plt.figure(figsize=(20,4*rows))\n",
    "    for index, (image, label) in enumerate(zip(x.T[selection,:], y.T[selection,:])):\n",
    "        plt.subplot(rows, cols, index+1)\n",
    "        plt.imshow(np.reshape(image, shape), cmap=plt.cm.gray)\n",
    "        plt.title('Sample %i\\n Label %i\\n' % (selection[index],label), fontsize = 12)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "def load_mnist(data_home):\n",
    "    \"\"\"\n",
    "    Loads the mnist dataset, prints the shape of the dataset and \n",
    "    returns the array with the images, the array with associated labels \n",
    "    and the shape of the images.     \n",
    "    Parameters: \n",
    "    data_home -- Absolute path to the DATA_HOME  \n",
    "    \n",
    "    Returns:\n",
    "    x -- array with images of shape (784,m) where m is the number of images\n",
    "    y -- array with associated labels with shape (1,m) where m is the number of images\n",
    "    shape -- (28,28)\n",
    "    \"\"\"\n",
    "    mnist = fetch_openml(name='mnist_784', version=1, cache=True, data_home=data_home)\n",
    "    x, y = mnist['data'].T, np.array(mnist['target'], dtype='int').T\n",
    "    m = x.shape[1]\n",
    "    y = y.reshape(1,m)\n",
    "    print(\"Loaded MNIST original:\")\n",
    "    print(\"Image Data Shape\" , x.shape)\n",
    "    print(\"Label Data Shape\", y.shape)\n",
    "    return x,y,(28,28)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data and bring it in the correct shape\n",
    "\n",
    "Split the data into training set and test set.\n",
    "We use the scikit-learn function 'train_test_split' and use 20% of the samples as test data.\n",
    "\n",
    "Furthermore, we bring the input data (x) into the shape (n,m) where n is the number of input features and m the number of samples.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def prepare_train_test(x, y, test_size=0.20):\n",
    "    \"\"\"\n",
    "    Split the dataset consisting of an array of images (shape (m,n)) and an array of labels (shape (n,))\n",
    "    into train and test set.\n",
    "    \n",
    "    Parameters:\n",
    "    x -- Array of images of shape (n,m) where m is the number of samples\n",
    "    y -- Array of labels of shape (m,) where m is the number of samples\n",
    "    test_size -- fraction of samples to reserve as test sample\n",
    "    \n",
    "    Returns:\n",
    "    x_train -- list of images of shape (n,m1) used for training\n",
    "    y_train -- list of labels of shape (1,m1) used for training\n",
    "    x_test -- list of images of shape (n,m2) used for testing\n",
    "    y_test -- list of labels of shape (1,m2) used for testing\n",
    "    \"\"\"\n",
    "    # split \n",
    "    # We use the functionality of sklearn which assumes that the samples are enumerated with the first index \n",
    "    x_train, x_test, y_train, y_test = train_test_split(x.T, y.T, test_size=0.20, random_state=1)\n",
    "\n",
    "    # reshape - transpose back the output obtained from the train_test_split-function\n",
    "    x_train = x_train.T\n",
    "    x_test = x_test.T\n",
    "    m_train = x_train.shape[1]\n",
    "    m_test = x_test.shape[1]\n",
    "    y_train=y_train.reshape(1,m_train)\n",
    "    y_test=y_test.reshape(1,m_test)\n",
    "\n",
    "    print(\"Shape training set: \", x_train.shape, y_train.shape)\n",
    "    print(\"Shape test set:     \", x_test.shape, y_test.shape)\n",
    "    \n",
    "    return x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Normalisation\n",
    "\n",
    "Normalize the data - apply min/max normalization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def normalize(x_train,x_test):\n",
    "    \"\"\"\n",
    "    Applies min/max-normalizes - min and max values computed from the training set.\n",
    "    Common min and max values for all features are used.\n",
    "    \n",
    "    Parameters:\n",
    "    x_train -- Array of training samples of shape (n,m1) where n,m1 are the number of features and samples, respectively.  \n",
    "    x_test -- Array of test samples of shape (n,m2) where n,m2 are the number of features and samples, respectively. \n",
    "    \n",
    "    Returns:\n",
    "    The arrays with the normalized train and test samples.  \n",
    "    \"\"\"\n",
    "    ### START YOUR CODE ###    \n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ### END YOUR CODE ###\n",
    "    return x_train, x_test "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def predict(W, b, X):\n",
    "    '''\n",
    "    Compute the per class probabilities for all the m samples by using a softmax layer with parameters (W, b).\n",
    "    \n",
    "    Arguments:\n",
    "    W -- weights, a numpy array with shape (ny, nx) (with ny=10 for MNIST).\n",
    "    b -- biases, a numpy array with shape (ny,1)\n",
    "    X -- input data of size (nx,m)\n",
    "    \n",
    "    Returns:\n",
    "    A -- a numpy array of shape (ny,m) with the prediction probabilities for the digits.\n",
    "    ''' \n",
    "    ### START YOUR CODE ###\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ### END YOUR CODE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TEST Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.array([[1,-1],[0,1],[-1,1]]).reshape(3,2)\n",
    "b = np.array([0,0,0]).reshape(3,1)\n",
    "X = np.array([2, 3]).reshape(2,1)\n",
    "A = predict(W,b,X)\n",
    "Aexp = np.array([0.01587624,0.86681333,0.11731043]).reshape(A.shape)\n",
    "np.testing.assert_array_almost_equal(A,Aexp,decimal=8)\n",
    "np.testing.assert_array_almost_equal(np.sum(A, axis=0), 1.0, decimal=8)\n",
    "\n",
    "X = np.array([[2,-1,1,-1],[1,1,1,1]]).reshape(2,4)\n",
    "A = predict(W,b,X)\n",
    "Aexp = np.array([[0.46831053, 0.01321289, 0.21194156, 0.01321289],\n",
    " [0.46831053, 0.26538793, 0.57611688, 0.26538793],\n",
    " [0.06337894, 0.72139918, 0.21194156, 0.72139918]]\n",
    ")\n",
    "np.testing.assert_array_almost_equal(A,Aexp,decimal=8)\n",
    "np.testing.assert_array_almost_equal(np.sum(A, axis=0), np.ones(4,dtype='float'), decimal=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Function (Cross Entropy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def cost(Ypred, Y):\n",
    "    \"\"\"\n",
    "    Computes the cross entropy cost function for given predicted values and labels.\n",
    "    \n",
    "    Parameters:\n",
    "    Ypred -- prediction from softmax, a numpy array of shape (ny,m)\n",
    "    Y -- ground truth labels - a numpy array with shape (1,m) containing digits 0,1,...,9. \n",
    "    \n",
    "    Returns:\n",
    "    Cross Entropy Cost - a scalar value\n",
    "    \"\"\"\n",
    "    ### START YOUR CODE ###  \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ### END YOUR CODE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TEST Cross Entropy Cost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = np.array([1])\n",
    "Ypred = np.array([0.04742587,0.95257413]).reshape(2,1)\n",
    "J = cost(Ypred,Y)\n",
    "Jexp = 0.04858735\n",
    "np.testing.assert_almost_equal(J,Jexp,decimal=8)\n",
    "\n",
    "Y = np.array([1,1,1,0])\n",
    "Ypred = np.array([[1.79862100e-02, 6.69285092e-03, 4.74258732e-02, 9.99088949e-01],\n",
    "                  [9.82013790e-01, 9.93307149e-01, 9.52574127e-01, 9.11051194e-04]])\n",
    "Jexp = 0.01859102\n",
    "J = cost(Ypred,Y)\n",
    "np.testing.assert_almost_equal(J,Jexp,decimal=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update Rules for the Parameters\n",
    "\n",
    "Different update rules associated with the different cost functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot(y,n):\n",
    "    \"\"\"\n",
    "    Constructs a one-hot-vector from a given array of labels (shape (1,m), containing numbers 0,1,...,n-1) \n",
    "    and the number of classes n.\n",
    "    The resulting array has shape (n,m) and in row j and column i a '1' if the i-th sample has label 'j'. \n",
    "    \n",
    "    Parameters:\n",
    "    y -- labels, numpy array of shape (1,m) \n",
    "    n -- number of classes\n",
    "\n",
    "    Returns:\n",
    "    On-hot-encoded vector of shape (n,m)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START YOUR CODE ###\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ### START YOUR CODE ###\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test ##\n",
    "Y = np.array([1,3,0]).reshape(1,3)\n",
    "onehot_comp = onehot(Y,4)\n",
    "onehot_exp = np.array([[0,0,1],[1,0,0],[0,0,0],[0,1,0]]).reshape(4,3)\n",
    "np.testing.assert_almost_equal(onehot_exp,onehot_comp,decimal=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def gradient(X, Y, A):\n",
    "    \"\"\"\n",
    "    Computes the update of the weights and bias - by using the cross entropy cost. \n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data of size (nx,m)\n",
    "    Y -- output labels - a numpy array with shape (1,m).\n",
    "    A -- predicted scores - a numpy array with shape (ny,m) \n",
    "    \n",
    "    Returns:\n",
    "    gradJ -- dictionary with the gradient w.r.t. W (key \"dW\" with shape (ny,nx)) and w.r.t. b (key \"db\" with shape (ny,1))\n",
    "    \"\"\"\n",
    "    ### START YOUR CODE ### \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ### END YOUR CODE ###\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the Calculation of the Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.array([[1,-1],[0,1],[-1,1]]).reshape(3,2)\n",
    "b = np.array([0,0,0]).reshape(3,1)\n",
    "X = np.array([[2,-1,1,-1],[1,1,1,1]]).reshape(2,4)\n",
    "A = predict(W,b,X)\n",
    "\n",
    "Y = np.array([1,1,1,1]).reshape(1,4)\n",
    "\n",
    "gradJ = gradient(X,Y,A)\n",
    "dW = gradJ['dW']\n",
    "db = gradJ['db']\n",
    "dWexp = np.array([[ 0.28053421,0.17666947],\n",
    "                  [-0.00450948,-0.60619918],\n",
    "                  [-0.27602473,0.42952972]]).reshape(3,2)\n",
    "dbexp = np.array([0.17666947,-0.60619918,0.42952972]).reshape(3,1)\n",
    "np.testing.assert_array_almost_equal(dW,dWexp,decimal=8)\n",
    "np.testing.assert_array_almost_equal(db,dbexp, decimal=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics for measuring the performance of the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def error_rate(Ypred, Y):\n",
    "    \"\"\"\n",
    "    Compute the error rate defined as the fraction of misclassified samples.\n",
    "    \n",
    "    Arguments:\n",
    "    Ypred -- Predicted label, a numpy array of size (1,m)\n",
    "    Y -- ground truth labels, a numpy array with shape (1,m)\n",
    "\n",
    "    Returns:\n",
    "    error_rate -- an array of shape (1,m)\n",
    "    \"\"\"\n",
    "    Ypredargmax = np.argmax(Ypred, axis=0)\n",
    "    return np.sum(Y != Ypredargmax) / Y.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize and Optimize (Learn)\n",
    "\n",
    "#### Initialize Parameters\n",
    "\n",
    "First we provide a utility method to generate properly intialized parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def initialize_params(nx, ny, random=False):\n",
    "    \"\"\"\n",
    "    This function provides initialized parameters: a weights matrix and a bias vector. \n",
    "    \n",
    "    Argument:\n",
    "    nx -- number of input features\n",
    "    ny -- number of output dimensions (number of different labels)\n",
    "    random -- if set to True standard normal distributed values are set for the weights; otherwise zeros are used.\n",
    "    \n",
    "    Returns:\n",
    "    w -- initialized weights matrix of shape (ny,nx)\n",
    "    b -- initialized bias vector of shape (ny,1) - always initialized with zeros\n",
    "    \"\"\"\n",
    "    if random:\n",
    "        w = np.random.randn(*(ny,nx)) / np.sqrt(nx)\n",
    "    else:\n",
    "        w = np.zeros((ny,nx))\n",
    "    \n",
    "    b = 0.0\n",
    "    \n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metrics Class\n",
    "\n",
    "For not littering the optimization loop with code to keep track of the learning results over the epochs we defined a suitable metrics object that keeps all the data (cost function, classification error vs epochs). It also provides utility methods for updating, printing values or plotting the learning curves.\n",
    "\n",
    "It is defined as python class the metrics object then needs to be instantiated from. It means that some small knowledge about object-oriented programming is needed here.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metrics():\n",
    "    \"\"\"\n",
    "    Allows to collect statistics (such as classification error or cost) that are of interest over the course of training\n",
    "    and for creating learning curves that are a useful tool for analyzing the quality of the learning.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, cost, smooth=False):\n",
    "        \"\"\"\n",
    "        Constructor for a metrics object. \n",
    "        Initializes all the statistics to track in form of python lists.\n",
    "        \n",
    "        Parameters:\n",
    "        cost -- cost function to use (a python function)\n",
    "        smooth -- if set to true updates learning curve after each training step and also provides learning curves \n",
    "        smoothed over the epoch  \n",
    "        \"\"\"\n",
    "        self.epochs = []\n",
    "        self.smooth = smooth\n",
    "        self.train_costs_last = []\n",
    "        self.test_costs_last = []\n",
    "        self.train_errors_last = []\n",
    "        self.test_errors_last = []\n",
    "        self.stepsize_w_last = []\n",
    "        self.stepsize_b_last = []\n",
    "        if self.smooth:\n",
    "            self.train_costs_smoothed = []\n",
    "            self.test_costs_smoothed = []\n",
    "            self.train_errors_smoothed = []\n",
    "            self.test_errors_smoothed = []\n",
    "            self.stepsize_w_smoothed = []\n",
    "            self.stepsize_b_smoothed = []\n",
    "\n",
    "        self.cost_function = cost\n",
    "        self.init_epoch()\n",
    "\n",
    "            \n",
    "    def init_epoch(self):\n",
    "        self.train_costs_epoch = []\n",
    "        self.test_costs_epoch = []\n",
    "        self.train_errors_epoch = []\n",
    "        self.test_errors_epoch = []\n",
    "        self.stepsize_w_epoch = []\n",
    "        self.stepsize_b_epoch = []\n",
    "        \n",
    "        \n",
    "    def update_epoch(self, epoch):\n",
    "        \"\"\"\n",
    "        Computes the average of the metrics over the epoch and adds the result to the per epoch history\n",
    "        \n",
    "        Parameters:\n",
    "        epoch -- the epoch to add to the per epoch cache\n",
    "        \"\"\"\n",
    "        self.epochs.append(epoch)\n",
    "        if self.smooth:\n",
    "            self.train_costs_smoothed.append(np.mean(self.train_costs_epoch))\n",
    "            self.test_costs_smoothed.append(np.mean(self.test_costs_epoch))\n",
    "            self.train_errors_smoothed.append(np.mean(self.train_errors_epoch))\n",
    "            self.test_errors_smoothed.append(np.mean(self.test_errors_epoch))\n",
    "            self.stepsize_w_smoothed.append(np.mean(self.stepsize_w_epoch))\n",
    "            self.stepsize_b_smoothed.append(np.mean(self.stepsize_b_epoch))                    \n",
    "\n",
    "        self.train_costs_last.append(self.train_costs_epoch[-1])\n",
    "        self.test_costs_last.append(self.test_costs_epoch[-1])\n",
    "        self.train_errors_last.append(self.train_errors_epoch[-1])\n",
    "        self.test_errors_last.append(self.test_errors_epoch[-1])\n",
    "        self.stepsize_w_last.append(self.stepsize_w_epoch[-1])\n",
    "        self.stepsize_b_last.append(self.stepsize_b_epoch[-1])                    \n",
    "        \n",
    "        self.init_epoch()\n",
    "    \n",
    "        \n",
    "    def update_iteration(self, ypred_train, y_train, ypred_test, y_test, dw, db):\n",
    "        \"\"\"\n",
    "        Allows to update the statistics to be tracked for a new epoch.\n",
    "        The cost is computed by using the function object passed to the constructor.\n",
    "        \n",
    "        Parameters:\n",
    "        epoch -- Epoch\n",
    "        ypred_train -- predicted values on the training samples, a numpy array of shape (1,m1)\n",
    "        y_train -- ground truth labels associated with the training samples, a numpy array of shape (1,m1)\n",
    "        ypred_test -- predicted values on the test samples, a numpy array of shape (1,m2)\n",
    "        y_test -- ground truth labels associated with the test samples, a numpy array of shape (1,m2)\n",
    "        dw -- some lenght measure for the gradient w.r.t. the weights, a numpy array of shape (1,n)\n",
    "        db -- gradient w.r.t. the bias, a scalar\n",
    "        \"\"\"\n",
    "        Jtrain = self.cost_function(ypred_train, y_train)\n",
    "        Jtest = self.cost_function(ypred_test, y_test)\n",
    "        train_error = error_rate(ypred_train, y_train)\n",
    "        test_error = error_rate(ypred_test, y_test)\n",
    "\n",
    "        self.train_costs_epoch.append(Jtrain)\n",
    "        self.test_costs_epoch.append(Jtest)\n",
    "        self.train_errors_epoch.append(train_error)\n",
    "        self.test_errors_epoch.append(test_error)\n",
    "        self.stepsize_w_epoch.append(dw)\n",
    "        self.stepsize_b_epoch.append(db)\n",
    "        \n",
    "        \n",
    "    def print_latest_errors(self):\n",
    "        print (\"Train/test error after epoch %i: %f, %f\" %(self.epochs[-1], self.train_errors_last[-1], self.test_errors_last[-1]))\n",
    "\n",
    "    def print_latest_costs(self):\n",
    "        print (\"Train/test cost after epoch %i: %f, %f\" %(self.epochs[-1], self.train_costs_last[-1], self.test_costs_last[-1]))\n",
    "\n",
    "    def plot_cost_curves(self, ymin=None, ymax=None, smooth=True):\n",
    "        plt.semilogy(self.epochs, self.train_costs_last, \"b-\", label=\"train\")\n",
    "        plt.semilogy(self.epochs, self.test_costs_last, \"r-\", label=\"test\")\n",
    "        if self.smooth and smooth:\n",
    "            plt.semilogy(self.epochs, self.train_costs_smoothed, \"b--\", label=\"train_smoothed\")\n",
    "            plt.semilogy(self.epochs, self.test_costs_smoothed, \"r--\", label=\"test_smoothed\")\n",
    "        plt.ylabel('Cost')\n",
    "        plt.xlabel('Epochs')\n",
    "        xmax = self.epochs[-1]\n",
    "        if not ymin:\n",
    "            ymin = min(max(1e-5,np.min(self.train_costs_last)),max(1e-5,np.min(self.test_costs_last))) * 0.8\n",
    "        if not ymax:\n",
    "            ymax = max(np.max(self.train_costs_last),np.max(self.test_costs_last)) * 1.2\n",
    "        plt.axis([0,xmax,ymin,ymax])\n",
    "        plt.legend()\n",
    "        plt.show()        \n",
    "    \n",
    "    def plot_error_curves(self, ymin=None, ymax=None, smooth=True):\n",
    "        plt.semilogy(self.epochs, self.train_errors_last, \"b\", label=\"train\")\n",
    "        plt.semilogy(self.epochs, self.test_errors_last, \"r\", label=\"test\")\n",
    "        if self.smooth and smooth:\n",
    "            plt.semilogy(self.epochs, self.train_errors_smoothed, \"b--\", label=\"train_smoothed\")\n",
    "            plt.semilogy(self.epochs, self.test_errors_smoothed, \"r--\", label=\"test_smoothed\")\n",
    "        plt.ylabel('Errors')\n",
    "        plt.xlabel('Epochs')\n",
    "        xmax = self.epochs[-1]\n",
    "        if not ymin:\n",
    "            ymin = min(max(1e-5,np.min(self.train_errors_last)),max(1e-5,np.min(self.test_errors_last))) * 0.8\n",
    "        if not ymax:\n",
    "            ymax = max(np.max(self.train_errors_last),np.max(self.test_errors_last)) * 1.2\n",
    "        plt.axis([0,xmax,ymin,ymax])\n",
    "        plt.legend()\n",
    "        plt.show()        \n",
    "\n",
    "    def plot_stepsize_curves(self, ymin=None, ymax=None, smooth=True):\n",
    "        plt.semilogy(self.epochs, self.stepsize_w_last, label=\"dw\")\n",
    "        plt.semilogy(self.epochs, self.stepsize_b_last, label=\"db\")\n",
    "        if self.smooth and smooth:\n",
    "            plt.semilogy(self.epochs, self.stepsize_w_smoothed, label=\"dw--\")\n",
    "            plt.semilogy(self.epochs, self.stepsize_b_smoothed, label=\"db--\")\n",
    "        plt.ylabel('Step Sizes (dw,db)')\n",
    "        plt.xlabel('Epochs')\n",
    "        xmax = self.epochs[-1]\n",
    "        if not ymin:\n",
    "            ymin = min(max(1e-5,np.min(self.stepsize_w_last)),max(1e-5,np.min(self.stepsize_b_last))) * 0.8\n",
    "        if not ymax:\n",
    "            ymax = max(np.max(self.stepsize_w_last),np.max(self.stepsize_b_last)) * 1.2\n",
    "        plt.axis([0,xmax,ymin,ymax])\n",
    "        plt.legend()\n",
    "        plt.show()        \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniBatches():\n",
    "    \n",
    "    def __init__(self, x, y, batchsize):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        m = x.shape[1]\n",
    "        if not batchsize:\n",
    "            self.batchsize = m\n",
    "        else:\n",
    "            self.batchsize = batchsize\n",
    "        self.n = x.shape[0]\n",
    "        self.mb = int(m/batchsize)\n",
    "        self.indices = np.arange(m)\n",
    "        np.random.shuffle(self.indices)\n",
    "        self.ib = 0\n",
    "        \n",
    "    def number_of_batches(self):\n",
    "        return self.mb\n",
    "        \n",
    "    def next(self):\n",
    "        it = self.indices[self.ib*batchsize:(self.ib+1)*batchsize]\n",
    "        xbatch = self.x[:,it].reshape(self.n,batchsize)\n",
    "        ybatch = self.y[:,it].reshape(1,batchsize)\n",
    "        self.ib += 1\n",
    "        return xbatch, ybatch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def optimize(W, b, x_train, y_train, x_test, y_test, nepochs, alpha, batchsize=32, debug=True):\n",
    "    \"\"\"\n",
    "    This function optimizes W and b by running (mini-batch) gradient descent. It starts with the given \n",
    "    weights as initial values and then iteratively updates the parameters for nepochs number of times.\n",
    "    It returns the trained parameters as dictionary (keys \"W\" and \"b\") and various quantities \n",
    "    collected during learning in form of a Metrics object. You don't need to provide smoothingÂ within \n",
    "    the epoch so that training will be somewhat faster.\n",
    "    \n",
    "    Arguments:\n",
    "    W -- weights, a numpy array of size (ny,nx)\n",
    "    b -- biases, a numpy array with shape (ny,1) (with ny=10 for MNIST).\n",
    "    x_train -- input data for training of shape (nx,m1)\n",
    "    y_train -- ground-truth labels - a numpy array with shape (1,m1)\n",
    "    x_test -- input data for training of shape (nx,m2)\n",
    "    y_test -- ground-truth labels - a numpy array with shape (1,m2)\n",
    "    nepochs -- number of iterations of the optimization loop\n",
    "    alpha -- learning rate of the gradient descent update rule\n",
    "    batchsize -- batch size, defaults to 32\n",
    "    debug -- if true prints training and test error values after each epoch. Defaults to True.\n",
    "    \n",
    "    Returns:\n",
    "    params -- dictionary containing the (final) weights w and bias b\n",
    "    metrics -- contain the information about the learning curves\n",
    "    \"\"\" \n",
    "    metrics = Metrics(cost = cost, smooth=False)\n",
    "\n",
    "    m = x_train.shape[1] # number of samples\n",
    "    nx = x_train.shape[0] # number of input features\n",
    "    mb = int(m/batchsize) # number of mini-batches\n",
    "    print(\"Optimisation with batchsize %i and %i number of batches per epoch.\"%(batchsize,mb))\n",
    "    \n",
    "    # compute and set the initial values for the metrics curves\n",
    "    ypred_train = predict(W,b,x_train)    \n",
    "    ypred_test = predict(W,b,x_test)    \n",
    "    metrics.update_iteration(ypred_train, y_train, ypred_test, y_test, 0, 0)\n",
    "    metrics.update_epoch(0)\n",
    "    \n",
    "    # Loop over the epochs    \n",
    "    for i in range(nepochs):\n",
    "                \n",
    "        # prepare shuffled mini-batches for this epoch\n",
    "        batches = MiniBatches(x_train, y_train, batchsize)\n",
    "        \n",
    "        ### START YOUR CODE ### \n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        ### END YOUR CODE ### \n",
    "            \n",
    "        if debug:\n",
    "            metrics.print_latest_errors()\n",
    "        \n",
    "    metrics.print_latest_costs()\n",
    "    metrics.print_latest_errors()\n",
    "\n",
    "    return {\"W\": W, \"b\": b}, metrics    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Training for Specific Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# preparing the data\n",
    "x,y, shape = load_mnist(data_home)\n",
    "x_train1, x_test1, y_train, y_test = prepare_train_test(x, y, test_size=0.20)\n",
    "x_train,x_test = normalize(x_train1,x_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### START YOUR CODE ### -- try different settings, ...\n",
    "\n",
    "learning_rate = 0.05\n",
    "nepochs = 200\n",
    "batchsize = 256\n",
    "W,b = initialize_params(28*28, 10, random=True)\n",
    "params, metrics = optimize(W, b, x_train, y_train, x_test, y_test, nepochs=nepochs, alpha = learning_rate, batchsize=batchsize, debug=True)\n",
    "\n",
    "### END YOUR CODE ### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Learning Curves\n",
    "\n",
    "Cost <br>\n",
    "Error Rate <br>\n",
    "Learning Speed (Lenght of Parameter Change)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metrics.plot_cost_curves()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metrics.plot_error_curves()\n",
    "metrics.print_latest_errors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "metrics.plot_stepsize_curves()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Describe your Findings for Exercise 1b\n",
    "\n",
    "By running the training with different settings for learning rate, number of epochs, batch size explore which combination is best suited to obtain good test performance. Keep an eye on random estimates for the error rates due to random parameter initialisation and randomly shuffled mini-batches. \n",
    "\n",
    "Specify your choice of these hyper-parameters and justify why you consider your choice best suited."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### YOUR FINDINGS ...\n",
    "\n",
    "\n",
    "\n",
    "...\n",
    "\n",
    "\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the Misclassified Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y_pred = predict(params['W'], params['b'], x_test)\n",
    "yhat = np.argmax(y_pred, axis=0)\n",
    "indices = np.where(yhat != y_test)[1]\n",
    "print(len(indices))\n",
    "\n",
    "plot_digits(x_test, y_test, indices[0:25], shape)\n",
    "print(y_test[:,indices[0:25]])\n",
    "print(yhat[indices[0:25]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the Trained Weights as Image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = params['W']\n",
    "biases = params['b']\n",
    "cols = 5\n",
    "rows = 2\n",
    "plt.figure(figsize=(20,4*rows))\n",
    "for i in range(10):\n",
    "    plt.subplot(rows, cols, i+1)\n",
    "    plt.imshow(np.reshape(weights[i], (28,28)), cmap=plt.cm.gray)\n",
    "    plt.title('Digit %i'%i, fontsize = 12)\n",
    "\n",
    "plt.figure(figsize=(20,4))\n",
    "plt.plot(range(10), [biases[i] for i in range(10)], '+')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
