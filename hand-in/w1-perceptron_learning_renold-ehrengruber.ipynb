{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron Learning Rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hand-In of Group **13**, Jonathan Ehrengruber (jonathan.ehrengruber@students.fhnw.ch), Christian Renold  (christian.renold@hslu.ch)\n",
    "\n",
    "Last revision: Martin Melchior - 18.09.2019\n",
    "\n",
    "In this exercise you implement the perceptron learning rule. Then you apply it to linearly separable data (the data can be generated on the fly) and you can convince yourself that your system is properly implemented and has found the separating line. \n",
    "\n",
    "Some emphasis should be given to properly handle numpy arrays. These will be much more extensively used in upcoming exercises of later weeks. So, we recommend to take a serious glance at them.\n",
    "\n",
    "\n",
    "### Preparation of the Data\n",
    "\n",
    "Instead of providing a fixed input dataset, we here generate it randomly.\n",
    "For easier comparison, we want to make sure that the same data is produced. Therefore, we set a random seed (set to 1 below).\n",
    "\n",
    "The data will be generated in form of a 2d array, the first index enumerating the dimensions (rows, in the 2d case index 0 and 1), the second enumerating the samples (columns). \n",
    "\n",
    "Furthermore, we provide a suitable plotting utility that allows you to inspect the generated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def prepare_data(m,m1,a,s,width=0.6,eps=0.5, seed=1):\n",
    "    \"\"\"\n",
    "    Generates a random linearly separable 2D test set and associated labels (0|1).\n",
    "    The x-values are distributed in the interval [-0.5,0.5]. \n",
    "    With the parameters a,s you can control the line that separates the two classes. \n",
    "    This turns out to be the line with the widest corridor between the two classes (with width 'width').\n",
    "    If the random seed is set, the set will always look the same for given input parameters. \n",
    "    \n",
    "    Arguments:\n",
    "    a -- y-intercept of the seperating line\n",
    "    s -- slope of the separating line\n",
    "    m -- number of samples\n",
    "    m1 -- number of samples labelled with '1'\n",
    "    width -- width of the corridor between the two classes\n",
    "    eps -- measure for the variation of the samples in x2-direction\n",
    "    \n",
    "    Returns:\n",
    "    x -- generated 2D data of shape (2,n)\n",
    "    y -- labels (0 or 1) of shape (1,n)\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    idx = np.random.choice(m, m1, replace=False)\n",
    "    y = np.zeros(m, dtype=int).reshape(1,m)\n",
    "    y[0,idx] = 1\n",
    "    \n",
    "    x = np.random.rand(2,m).reshape(2,m) # random numbers uniformly distributed in [0,1]\n",
    "    x[0,:]-= 0.5\n",
    "    idx1 = y[0,:]==1\n",
    "    idx2 = y[0,:]==0\n",
    "    x[1,idx1] = (a+s*x[0,idx1]) + (width/2+eps*x[1,idx1])\n",
    "    x[1,idx2] = (a+s*x[0,idx2]) - (width/2+eps*x[1,idx2])\n",
    "    \n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def line(a, s, n=100):\n",
    "    \"\"\"    \n",
    "    Returns a line 2D array with x and y=a+s*x.\n",
    "    \n",
    "    Parameters:\n",
    "    a -- intercept\n",
    "    s -- slope\n",
    "    n -- number of points\n",
    "    \n",
    "    Returns:\n",
    "    2d array of shape (n,2) \n",
    "    \"\"\"\n",
    "    x = np.linspace(-0.5, 0.5, n)\n",
    "    l = np.array([x,a+s*x]).reshape(2,n)\n",
    "    return l\n",
    "\n",
    "def plot(x, y, params_best=None, params_before=None, params_after=None, misclassified=None, selected=None):\n",
    "    \"\"\"\n",
    "    Plot the 2D data provided in form of the x-array. \n",
    "    Use markers depending on the label ('1 - red cross, 0 - blue cross').\n",
    "    Optionally, you can pass tuples with parameters for a line (a: y-intercept, s: slope) \n",
    "    * params_best: ideal separating line (green dashed) \n",
    "    * params: predicted line (magenta)\n",
    "    Finally, you can also mark single points:\n",
    "    * misclassified: array of misclassified points (blue circles)\n",
    "    * selected: array of selected points (green filled circles)\n",
    "    \n",
    "    Parameters:\n",
    "    x -- 2D input dataset of shape (2,n)\n",
    "    y -- ground truth labels of shape (1,n)\n",
    "    params_best -- parameters for the best separating line\n",
    "    params -- any line parameters\n",
    "    misclassified -- array of points to be marked as misclassified\n",
    "    selected -- array of points to be marked as selected\n",
    "    \"\"\"\n",
    "    idx1 = y[0,:]==1\n",
    "    idx2 = y[0,:]==0\n",
    "    plt.plot(x[0,idx1], x[1,idx1], 'r+', label=\"label 1\")\n",
    "    plt.plot(x[0,idx2], x[1,idx2], 'b+', label=\"label 0\")    \n",
    "    if not params_best is None:\n",
    "        a = params_best[0]\n",
    "        s = params_best[1]\n",
    "        l = line(a,s)\n",
    "        plt.plot(l[0,:], l[1,:], 'g--')\n",
    "    if not params_before is None:\n",
    "        a = params_before[0]\n",
    "        s = params_before[1]\n",
    "        l = line(a,s)\n",
    "        plt.plot(l[0,:], l[1,:], 'm--')\n",
    "    if not params_after is None:\n",
    "        a = params_after[0]\n",
    "        s = params_after[1]\n",
    "        l = line(a,s)\n",
    "        plt.plot(l[0,:], l[1,:], 'm-')\n",
    "    if not misclassified is None:\n",
    "        plt.plot(x[0,misclassified], x[1,misclassified], 'o', label=\"misclassified\")\n",
    "    if not selected is None:\n",
    "        plt.plot(x[0,selected], x[1,selected], 'oy', label=\"selected\")\n",
    "        \n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate and Plot a Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x,y = prepare_data(200,100,0,0.5,width=0.3,eps=0.5, seed=1)\n",
    "plot(x, y, params_before=(0,0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters for the decision boundary\n",
    "\n",
    "Here, you should implement a function that translates the weights vector $(w_1,w_2)$ and the bias $b$ into parameters of a straight line ( $x_2 = a + s \\cdot x_1$ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def lineparams(weight, bias):\n",
    "    \"\"\"\n",
    "    Translates the weights vector and the bias into line parameters with a x2-intercept 'a' and a slope 's'.\n",
    "\n",
    "    Parameters:\n",
    "    weight -- weights vector of shape (1,2)\n",
    "    bias -- bias (a number)\n",
    "    \n",
    "    Returns:\n",
    "    a -- x2-intercept\n",
    "    s -- slope of the line in the (x1,x2)-plane\n",
    "    \"\"\"\n",
    "    ### START YOUR CODE ###\n",
    "    \n",
    "    w1, w2 = weight[0]\n",
    "    a = -bias / w2\n",
    "    s = -w1 / w2\n",
    "    \n",
    "    ### END YOUR CODE ###\n",
    "    return a,s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement the Perceptron Learning Algorithm\n",
    "\n",
    "by implementing the functions\n",
    "* predict\n",
    "* update\n",
    "* select_datapoint\n",
    "* train\n",
    "\n",
    "Follow the descriptions of these functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def predict(x,w,b):\n",
    "    \"\"\"\n",
    "    Computes the predicted value for a perceptron (single LTU).\n",
    "    \n",
    "    Parameters:\n",
    "    x -- input dataset of shape (2,m)\n",
    "    w -- weights vector of shape (1,2)\n",
    "    b -- bias (a number)\n",
    "    \n",
    "    Returns:\n",
    "    y -- prediction of a perceptron (single LTU) of shape (1,m)\n",
    "    \"\"\"\n",
    "    ### START YOUR CODE ###\n",
    "    yc = w.dot(x) + b\n",
    "    y = np.heaviside(yc, 1)\n",
    "    \n",
    "    ### END YOUR CODE ###\n",
    "    \n",
    "    return y\n",
    "\n",
    "def update(x,y,w,b,alpha=1.0):\n",
    "    \"\"\"\n",
    "    Performs an update step in accordance with the perceptron learning algorithm.\n",
    "    \n",
    "    Parameters:\n",
    "    x -- input data point of shape (2,1)\n",
    "    y -- true label ('ground truth') for the specified point\n",
    "    w -- weight vector of shape (1,2)\n",
    "    b -- bias (a number)\n",
    "    \n",
    "    Returns:\n",
    "    w1 -- updated weight vector\n",
    "    b1 -- updated bias\n",
    "    \"\"\"\n",
    "    ypred = predict(x,w,b)\n",
    "\n",
    "    ### START YOUR CODE ###\n",
    "    ydelta = ypred - y\n",
    "    w1 = w - alpha * ydelta * x\n",
    "    b1 = b - alpha * ydelta\n",
    "    ### END YOUR CODE ###\n",
    "\n",
    "    return w1, b1\n",
    "\n",
    "\n",
    "def select_datapoint(x, y, w, b):\n",
    "    \"\"\"\n",
    "    Identifies the misclassified data points and selects one of them.\n",
    "    In case all datapoints are correctly classified None is returned. \n",
    "\n",
    "    Parameters:\n",
    "    x -- input dataset of shape (2,m)\n",
    "    y -- ground truth labels of shape (1,m)\n",
    "    w -- weights vector of shape (1,2)\n",
    "    b -- bias (a number)\n",
    "    \n",
    "    Returns:\n",
    "    x1 -- one of the wrongly classified datapoint (of shape (2,1))\n",
    "    y1 -- the associated true label\n",
    "    misclasssified -- array with indices of wrongly classified datapoints or empty array\n",
    "    \"\"\"\n",
    "    ypred = predict(x,w,b)\n",
    "    wrong_mask = (ypred != y)[0]\n",
    "    misclassified = np.where(wrong_mask)[0]\n",
    "    if len(misclassified)>0:\n",
    "        x1 = x[:,misclassified[0]]\n",
    "        y1 = y[0,misclassified[0]]\n",
    "        return x1, y1, misclassified\n",
    "    return None, None, []\n",
    "\n",
    "def train(weight_init, bias_init, x, y, alpha=1.0, debug=False, params_best=None, max_iter=1000):\n",
    "    \"\"\"\n",
    "    Trains the perceptron (single LTU) for the given data x and ground truth labels y\n",
    "    by using the perceptron learning algorithm with learning rate alpha (default is 1.0).\n",
    "    The max number of iterations is limited to 1000.\n",
    "    \n",
    "    Optionally, debug output can be provided in form of plots with showing the effect \n",
    "    of the update (decision boundary before and after the update) provided at each iteration.    \n",
    "    \n",
    "    Parameters:\n",
    "    weight_init -- weights vector of shape (1,2)\n",
    "    bias_init -- bias (a number)\n",
    "    x -- input dataset of shape (2,m)\n",
    "    y -- ground truth labels of shape (1,m)\n",
    "    alpha -- learning rate\n",
    "    debug -- flag for whether debug information should be provided for each iteration\n",
    "    params_best -- needed if debug=True for plotting the true decision boundary\n",
    "    \n",
    "    Returns:\n",
    "    weight -- trained weights\n",
    "    bias -- trained bias\n",
    "    misclassified_counts -- array with the number of misclassifications at each iteration\n",
    "    \"\"\"\n",
    "    weight = weight_init\n",
    "    bias = bias_init\n",
    "    iterations = 0\n",
    "    misclassified_counts = [] # we track them to show how the system learned in the end \n",
    "    \n",
    "    ### START YOUR CODE ###\n",
    "    params_before = None\n",
    "    params_after = None\n",
    "    \n",
    "    # Getting a more ideal data structure\n",
    "    xt = x.T\n",
    "    yt = y.T\n",
    "    xtest = 1\n",
    "    while iterations<=max_iter and xtest is not None:\n",
    "        for xi, yi in zip(xt, yt):\n",
    "            weight, bias = update(xi, yi, weight, bias, alpha)\n",
    "        \n",
    "        xtest, ytest, misclassified = select_datapoint(x, y, weight, bias)\n",
    "        misclassified_counts.append(len(misclassified))\n",
    "        \n",
    "        if debug:\n",
    "            params_before = params_after\n",
    "            params_after = lineparams(weight, bias)\n",
    "            selected = None if xtest is None else np.array([misclassified[0]])\n",
    "            plot(x,y,params_best=params_best, params_before=params_before, params_after=params_after, misclassified=misclassified, selected=selected)\n",
    "        \n",
    "        iterations += 1\n",
    "    ### END YOUR CODE ###\n",
    "        \n",
    "    return weight, bias, misclassified_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Auxiliary Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def weights_and_bias(a,s):\n",
    "    \"\"\"\n",
    "    Computes weights vector and bias from line parameters x2-intercept and slope.\n",
    "    \"\"\"\n",
    "    w1 = - s\n",
    "    w2 = 1.0\n",
    "    weight = np.array([w1,w2]).reshape(1,2)\n",
    "    bias = - a\n",
    "    return weight, bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Your Implementation\n",
    "\n",
    "1/ Prepare the dataset by using the prepare_data function defined above and plot it. Use the parameters specified below (a=1, s=2, n=100, n1=50).\n",
    "\n",
    "2/ Run the training with the default learning rate (alpha=1).\n",
    "Paste the plots with the situation at the start and with the situation at the end of the training in a text document.\n",
    "Paste also the start parameters (weight and bias) and trained parameters.\n",
    "\n",
    "3/ Create a plot with the number of mis-classifications vs iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1/ Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "m = 100\n",
    "m1 = 50\n",
    "a = 1\n",
    "s = 2\n",
    "x,y = prepare_data(m,m1,a,s, seed=1)\n",
    "\n",
    "params_best = (a,s)\n",
    "weight_best, bias_best = weights_and_bias(a, s)\n",
    "print(\"weight: \", weight_best, \"  bias: \", bias_best)\n",
    "plot(x,y,params_best=params_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2/ Run the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a1 = 0\n",
    "s1 = 0\n",
    "alpha = 1.0\n",
    "\n",
    "weight1, bias1 = weights_and_bias(a1,s1)\n",
    "print(\"Initial Params: \",weight1,bias1)\n",
    "params = lineparams(weight1, bias1)\n",
    "print(params)\n",
    "plot(x,y,params_best, params)\n",
    "\n",
    "weight1,bias1,misclassified_counts = train(weight1, bias1, x, y)\n",
    "# weight1,bias1,misclassified_counts = train(weight1, bias1, x, y, debug=True, params_best=params_best)\n",
    "params = lineparams(weight1, bias1)\n",
    "print(\"Misclassified Counts: \", misclassified_counts)\n",
    "print(\"Iterations: \", len(misclassified_counts)-1)\n",
    "print(\"Trained Params: \", weight1,bias1)\n",
    "plot(x,y, params_best=params_best, params_after=params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3/ Create the plot with the misclassifications per iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nit = len(misclassified_counts)\n",
    "it = np.linspace(0,nit,nit)\n",
    "\n",
    "_ = plt.plot(it, misclassified_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment\n",
    "In this plot the iteration number (x-axis) is plotted against the number of misclassified points (y-axis). It can be seen that the number of misclassified go to 0 in the third iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: The function is copied from a previous ML course taught by M. Graber, 2018 @ FHNW and adapted by us\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "def plot_decision_regions(X, y, weights, bias, resolution=0.02):\n",
    "    markers = ('o', 'o', 'o', 'o', 'o')\n",
    "    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')\n",
    "    cmap = ListedColormap(colors[:len(np.unique(y))])\n",
    "    \n",
    "    # plot the decision surface\n",
    "    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    \n",
    "    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),\n",
    "                            np.arange(x2_min, x2_max, resolution))\n",
    "    \n",
    "    xx0 = np.array([bias]*len(xx1)).T\n",
    "    xx0 = np.expand_dims(xx1, axis=1)\n",
    "    test = np.array([xx1.ravel(), xx2.ravel()]).T\n",
    "    \n",
    "    ones = np.array([np.ones(test.shape[0])]).T\n",
    "    \n",
    "    Z = predict(test.T, weights, bias)\n",
    "    Z = Z.reshape(xx1.shape)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(6,6))\n",
    "    ax.contourf(xx1, xx2, Z, alpha=0.4, cmap=cmap)\n",
    "    \n",
    "    ax.set_xlim(xx1.min(), xx1.max())\n",
    "    ax.set_ylim(xx2.min(), xx2.max())\n",
    "    \n",
    "    # plot all samples\n",
    "    for idx, cl in enumerate(np.unique(y)):\n",
    "        ax.scatter(x=X[(y == cl)[:, 0], 0], y=X[(y == cl)[:, 0], 1], alpha=0.8, c=[cmap(idx)], \n",
    "                   marker=markers[idx], label=cl, edgecolor='none')\n",
    "        \n",
    "    _ = ax.set_xlabel('petal width')\n",
    "    _ = ax.set_ylabel('sepal width')\n",
    "    _ = ax.set_title('Decision region plot')\n",
    "    plt.figtext(.25, -.01, 'Decision region plot of petal width and sepal width')\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot_decision_regions(x.T, y.T, weight1, bias1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment\n",
    "As the plot above seems to be not that clear another plot was created. Here it can be seen clearly that the linear separation worked well. All points in the blue side are blue as it should be. The same can be said for the red points so the perceptron seems to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
